---
phase: 04-seo-&-performans-optimizasyonu
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/app/layout.tsx
  - src/app/sitemap.ts
  - src/app/robots.ts
  - src/lib/metadata/index.ts
  - .env.local
autonomous: true

must_haves:
  truths:
    - "Sitemap.xml accessible at /sitemap.xml with all 7 static routes"
    - "robots.txt accessible at /robots.txt allowing all bots with sitemap reference"
    - "Root layout has metadataBase set for absolute URL generation"
    - "Metadata constants defined for all service pages"
  artifacts:
    - path: "src/app/sitemap.ts"
      provides: "Build-time sitemap generation"
      exports: ["default function sitemap()"]
      contains: "MetadataRoute.Sitemap"
    - path: "src/app/robots.ts"
      provides: "Build-time robots.txt generation"
      exports: ["default function robots()"]
      contains: "MetadataRoute.Robots"
    - path: "src/lib/metadata/index.ts"
      provides: "Centralized metadata constants"
      exports: ["SERVICE_METADATA", "HOMEPAGE_METADATA"]
    - path: "src/app/layout.tsx"
      provides: "Root layout with metadataBase"
      contains: "metadataBase"
  key_links:
    - from: "src/app/sitemap.ts"
      to: "src/lib/metadata/index.ts"
      via: "import SERVICE_METADATA"
      pattern: "import.*SERVICE_METADATA"
    - from: "src/app/layout.tsx"
      to: "process.env.NEXT_PUBLIC_BASE_URL"
      via: "environment variable"
      pattern: "NEXT_PUBLIC_BASE_URL"
---

<objective>
Create SEO infrastructure: sitemap generation, robots.txt, metadata constants, and root layout with metadataBase.

Purpose: Establish the foundation for SEO by providing search engines with discoverable sitemap, proper crawling permissions, and centralized metadata management.
Output: sitemap.ts, robots.ts, src/lib/metadata/index.ts, updated root layout, .env.local with BASE_URL
</objective>

<execution_context>
@C:\Users\Omer\.claude\get-shit-done\workflows\execute-plan.md
@C:\Users\Omer\.claude\get-shit-done\templates\summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

@.planning/phases/04-seo-&-performans-optimizasyonu/04-CONTEXT.md
@.planning/phases/04-seo-&-performans-optimizasyonu/04-RESEARCH.md

@src/app/layout.tsx
@.env.local
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add NEXT_PUBLIC_BASE_URL environment variable</name>
  <files>.env.local</files>
  <action>
    Add NEXT_PUBLIC_BASE_URL to .env.local:
    - Set to https://mokadijital.com for production
    - Add comment about localhost for development
    - Place after NEXT_PUBLIC_N8N_WEBHOOK_URL

    DO NOT add NEXT_PUBLIC_BASE_URL to .env (gitignored .env.local only for production domain)
  </action>
  <verify>grep -q "NEXT_PUBLIC_BASE_URL" .env.local && echo "BASE_URL added"</verify>
  <done>.env.local contains NEXT_PUBLIC_BASE_URL=https://mokadijital.com</done>
</task>

<task type="auto">
  <name>Task 2: Create centralized metadata constants</name>
  <files>src/lib/metadata/index.ts</files>
  <action>
    Create src/lib/metadata/index.ts with:

    1. ServiceMetadata interface with: slug, title, description, keywords, ogImage, changeFrequency, priority

    2. HOMEPAGE_METADATA constant:
       - title: "Dijital Pazarlama Ajansı | Moka Dijital - Google İşletme, SEO, PMK"
       - description: "Google İşletme, SEO, PMK ve dijital pazarlama çözümleri ile işletmenizi büyütün. Yerel aramalarda öne çıkın."
       - keywords: ["dijital pazarlama", "seo ajansı", "google isletme", "potansiyel müşteri kazanımı"]
       - ogImage: "/og-image.jpg" (single fallback for all pages)
       - changeFrequency: "weekly"
       - priority: 1

    3. SERVICE_METADATA Record with all 6 services:
       - google-isletme: "Google İşletme Optimizasyonu Hizmeti Türkiye | Moka Dijital", description about profile optimization, reviews, photo management
       - seo: "SEO Hizmetleri | Yerel SEO & Global SEO | Moka Dijital", description about organic rankings, keyword analysis, backlink strategy
       - pmk: "Potansiyel Müşteri Kazanımı Hizmeti | n8n Otomasyon | Moka Dijital", description about n8n workflows, lead scoring, automation
       - web-gelistirme: "Web Sitesi Geliştirme Hizmeti | Next.js | Moka Dijital", description about custom web development, responsive design, fast loading
       - reklam: "Dijital Reklam Yönetimi Hizmeti | Google Ads | Moka Dijital", description about Google Ads, meta ads, ROAS optimization
       - otomasyon: "İş Akışı Otomasyonları Hizmeti | n8n | Moka Dijital", description about workflow automation, AI integration, efficiency

    All descriptions 150-160 characters with keyword-first placement.
    Use single /og-image.jpg for all services (deferred per-page images per CONTEXT).
  </action>
  <verify>
    grep -q "SERVICE_METADATA" src/lib/metadata/index.ts && grep -q "HOMEPAGE_METADATA" src/lib/metadata/index.ts && echo "Metadata constants created"
  </verify>
  <done>src/lib/metadata/index.ts exports HOMEPAGE_METADATA and SERVICE_METADATA with all 6 services defined</done>
</task>

<task type="auto">
  <name>Task 3: Create sitemap.ts</name>
  <files>src/app/sitemap.ts</files>
  <action>
    Create src/app/sitemap.ts that:
    - Imports MetadataRoute.Sitemap from 'next'
    - Imports SERVICE_METADATA from '@/lib/metadata'
    - Uses process.env.NEXT_PUBLIC_BASE_URL with fallback 'https://mokadijital.com'
    - Returns array with 7 entries:
      * Homepage: url=BASE_URL, lastModified=new Date(), changeFrequency='weekly', priority=1
      * Each service: url=`${BASE_URL}/hizmetler/{slug}`, using metadata from SERVICE_METADATA for changeFrequency and priority

    Use spread/map for service entries to keep code DRY.
  </action>
  <verify>
    grep -q "MetadataRoute.Sitemap" src/app/sitemap.ts && grep -q "SERVICE_METADATA" src/app/sitemap.ts && echo "Sitemap created"
  </verify>
  <done>src/app/sitemap.ts exports default function returning MetadataRoute.Sitemap with 7 routes</done>
</task>

<task type="auto">
  <name>Task 4: Create robots.ts</name>
  <files>src/app/robots.ts</files>
  <action>
    Create src/app/robots.ts that:
    - Imports MetadataRoute.Robots from 'next'
    - Uses process.env.NEXT_PUBLIC_BASE_URL with fallback 'https://mokadijital.com'
    - Returns:
      * rules: [{ userAgent: '*', allow: '/' }]
      * sitemap: `${BASE_URL}/sitemap.xml`

    Do NOT add any disallow rules (per CONTEXT decision).
  </action>
  <verify>
    grep -q "MetadataRoute.Robots" src/app/robots.ts && grep -q "sitemap" src/app/robots.ts && echo "Robots.txt created"
  </verify>
  <done>src/app/robots.ts exports default function returning MetadataRoute.Robots with allow all and sitemap reference</done>
</task>

<task type="auto">
  <name>Task 5: Update root layout with metadataBase and enhanced metadata</name>
  <files>src/app/layout.tsx</files>
  <action>
    Update src/app/layout.tsx:

    1. Add metadataBase to export const metadata:
       - metadataBase: new URL(process.env.NEXT_PUBLIC_BASE_URL || 'https://mokadijital.com')

    2. Update existing metadata fields:
       - title: Keep existing "Moka Dijital - Dönüşüm Odaklı Web Sitesi" (will be overridden by page-level generateMetadata)
       - description: Enhance to "Dijital pazarlama ajansı. Google İşletme, SEO, Potansiyel Müşteri Kazanımı ve web sitesi geliştirme hizmetleri."
       - Add keywords: ["moka dijital", "dijital pazarlama", "seo ajansı", "google isletme", "n8n otomasyon"]
       - Add openGraph with title, description, siteName: "Moka Dijital", locale: "tr_TR", type: "website"
       - Add metadataBase BEFORE other metadata (critical requirement per RESEARCH)

    DO NOT change existing Inter font configuration or lang="tr" attribute.
    DO NOT remove Header and Footer components.
  </action>
  <verify>
    grep -q "metadataBase" src/app/layout.tsx && grep -q "openGraph" src/app/layout.tsx && echo "Root layout updated"
  </verify>
  <done>src/app/layout.tsx has metadataBase set and enhanced metadata with openGraph tags</done>
</task>

</tasks>

<verification>
After all tasks complete:
1. Build project: `npm run build`
2. Check build output contains /sitemap.xml and /robots.txt in out/ directory
3. Verify sitemap accessible: `curl http://localhost:3000/sitemap.xml` (after npm run dev)
4. Verify robots.txt accessible: `curl http://localhost:3000/robots.txt`
5. Check sitemap contains all 7 URLs with correct priorities
6. Check robots.txt allows all bots and references sitemap
</verification>

<success_criteria>
- sitemap.ts generates valid sitemap.xml at build time with all 7 routes
- robots.ts generates valid robots.txt at build time with sitemap reference
- Root layout has metadataBase for absolute URL generation
- Metadata constants centralized in src/lib/metadata/index.ts
- .env.local has NEXT_PUBLIC_BASE_URL configured
</success_criteria>

<output>
After completion, create `.planning/phases/04-seo-&-performans-optimizasyonu/04-01-SUMMARY.md`
</output>
